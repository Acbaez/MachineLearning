{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import statistics "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 23)\n"
     ]
    }
   ],
   "source": [
    "# Read in datafiles\n",
    "training_data = np.loadtxt('./data/pa2train.txt')\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 23)\n"
     ]
    }
   ],
   "source": [
    "# Read in datafiles\n",
    "validation_data = np.loadtxt('./data/pa2validation.txt')\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 23)\n"
     ]
    }
   ],
   "source": [
    "# Read in datafiles\n",
    "test_data = np.loadtxt('./data/pa2test.txt')\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error(Y_pred, Y_label): \n",
    "    # Calculate Error Rate for Predicted Labels\n",
    "    error = [0 for x,y in zip(Y_pred,Y_label) if x != y]\n",
    "    error_rate = len(error)/len(Y_pred)\n",
    "    return error_rate\n",
    "\n",
    "# Function for calculating info gain\n",
    "def info_gain(subset, feature, threshold, label):\n",
    "    \n",
    "    num_samples = len(subset)\n",
    "    # Partition Subset into two sets v1, v2\n",
    "    v1, v2 = [x for x in subset if x[feature] < threshold], [x for x in subset if x[feature] > threshold]\n",
    "    v1_samples, v2_samples = len(v1), len(v2)\n",
    "    \n",
    "    # Find distribution of labels for both partitions\n",
    "    v1_default, v2_default = len([x for x in v1 if x[label] == 1]), len([x for x in v2 if x[label] == 0])\n",
    "    v1_no_default, v2_no_default = (v1_samples - v1_default), (v2_samples - v2_default)\n",
    "    \n",
    "    # Compare distribution of labels in both subsets, P(Z=z)\n",
    "    p_yes_threshold =  (v2_samples/num_samples)\n",
    "    p_no_threshold = (1 - p_yes_threshold)\n",
    "    \n",
    "    # Calculate conditional entropy for sboth partitions H(X|Z=z)\n",
    "    if v1_default == 0 or v1_no_default == 0: \n",
    "        if v1_samples == 0: \n",
    "            cond_entropy_no = 0\n",
    "        elif v1_default == 0: \n",
    "            cond_entropy_no = ((v1_no_default/len(v1))*math.log(v1_no_default/len(v1)))\n",
    "        else: \n",
    "            cond_entropy_no = ((v1_default/len(v1))*math.log(v1_default/len(v1)))\n",
    "    else:\n",
    "        cond_entropy_no = -(((v1_no_default/len(v1))*math.log(v1_no_default/len(v1)))+((v1_default/len(v1))*math.log(v1_default/len(v1))))\n",
    "        # Calculate conditional entropy for sboth partitions H(X|Z=z)\n",
    "    \n",
    "    if v2_default == 0 or v2_no_default == 0: \n",
    "        if v2_samples == 0: \n",
    "            cond_entropy_yes = 0\n",
    "        elif v2_default == 0: \n",
    "            cond_entropy_yes = ((v2_no_default/len(v2))*math.log(v2_no_default/len(v2)))\n",
    "        else: \n",
    "            cond_entropy_yes = ((v2_default/len(v2))*math.log(v2_default/len(v2)))\n",
    "    else:\n",
    "        cond_entropy_yes = -(((v2_default/len(v2))*math.log(v2_default/len(v2)))+((v2_no_default/len(v2))*math.log(v2_no_default/len(v2)))) \n",
    "    \n",
    "    # return overall conditional entropy H(X|Z)\n",
    "    return (cond_entropy_yes*(len(v2)/num_samples) + cond_entropy_no*(len(v1)/num_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function for Obtaining threshold values\n",
    "def get_thresholds(feature_vals):\n",
    "    thresholds = []\n",
    "    features = sorted(set(feature_vals))\n",
    "    for i in range(1,len(features)): \n",
    "        thresholds.append((features[i-1]+features[i])/2)\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_threshold(feature, threshold, data): \n",
    "    rows = data.shape[0]\n",
    "    cols = data.shape[1]\n",
    "    left_partition = np.empty((0,0))\n",
    "    right_partition = np.empty((0,0))\n",
    "    for i in range(rows): \n",
    "        if data[i,feature] < threshold: \n",
    "            if left_partition.shape == (0,0): \n",
    "                left_partition = np.array(data[i,:])\n",
    "                left_partition = left_partition.reshape((1,cols))\n",
    "            else: \n",
    "                # Add to \"No\" partition\n",
    "                left_partition = np.vstack((left_partition,data[i,:]))\n",
    "        elif data[i,feature] > threshold: \n",
    "            if right_partition.shape == (0,0): \n",
    "                right_partition = np.array(data[i,:])\n",
    "                right_partition = right_partition.reshape((1,cols))\n",
    "            else: \n",
    "                # Add to \"Yes\" partition\n",
    "                right_partition = np.vstack((right_partition,data[i,:]))\n",
    "    return right_partition, left_partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_decision_rule(training_samples): \n",
    "    # Pick feature, threshold pair that maxes info gain \n",
    "    split_rule = {}\n",
    "    for i in range(training_samples.shape[1]-1):\n",
    "        feature_dict = {}\n",
    "        # Obtain thresholds\n",
    "        thresholds = get_thresholds(training_samples[:,i])\n",
    "        for threshold in thresholds: \n",
    "            # Calculate info gain for threshold-feature pair\n",
    "            ig = info_gain(training_samples, i, threshold, training_samples.shape[1]-1)\n",
    "            # Append to dictionary \n",
    "            feature_dict[ig] = (i, threshold)\n",
    "            # Use feature-threshold pair with max info gain\n",
    "            max_ig = sorted(feature_dict.keys())[0]\n",
    "            feature_threshold = feature_dict[max_ig]\n",
    "            split_rule[max_ig] = feature_threshold      \n",
    "    # Find final split rule\n",
    "    split = split_rule[sorted(split_rule.keys())[0]]\n",
    "    return (split[0],split[1]), sorted(split_rule.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Decision Tree Node Class\n",
    "class decisionTreeNode: \n",
    "        \n",
    "    # Define constructor\n",
    "    def __init__(self, data):\n",
    "        self.children = []\n",
    "        self.pure = False\n",
    "        self.feature = 0\n",
    "        self.threshold = 0\n",
    "        self.data = data\n",
    "        self.predicted_label = None\n",
    "        self.entropy = float(0.0)\n",
    "        self.visited = False\n",
    "    \n",
    "    def setLabel(self, label): \n",
    "        self.predicted_label\n",
    "        \n",
    "        \n",
    "    def isPure(self, label): \n",
    "        isPure = False\n",
    "        labels = [x[label] for x in self.data]\n",
    "        # Check if labels for node are pure\n",
    "        if len(set(labels)) == 1: \n",
    "            isPure = True\n",
    "        return isPure\n",
    "\n",
    "# Define Decision Tree\n",
    "class decisionTree: \n",
    "    # Define Decision Tree Constructor\n",
    "    def __init__(self, training_data):\n",
    "        label_col = (training_data.shape[1] - 1)\n",
    "        self.root = decisionTreeNode(training_data)\n",
    "        self.impure_leaf_nodes = []\n",
    "        if self.root.isPure(label_col) == False: \n",
    "            self.impure_leaf_nodes.append(self.root)\n",
    "    \n",
    "    \n",
    "    # Function for building decision tree\n",
    "    def train(self, training_data): \n",
    "        label_index = (training_data.shape[1] - 1)\n",
    "        # Continue Algorithm until all leaf nodes are pure\n",
    "        while len(self.impure_leaf_nodes) != 0: \n",
    "            # Pick an impure node V and remove from list\n",
    "            parent_node = self.impure_leaf_nodes[-1]\n",
    "            self.impure_leaf_nodes.pop(-1)\n",
    "            split_rules = {}\n",
    "            \n",
    "            # Find decision rule for parent node\n",
    "            data = parent_node.data\n",
    "            split_rule, cond_entropy = find_decision_rule(data)\n",
    "            parent_node.feature = split_rule[0]\n",
    "            parent_node.threshold = split_rule[1]\n",
    "            \n",
    "            # Define subsets based on splitting rule\n",
    "            right_split, left_split = split_threshold(parent_node.feature, parent_node.threshold, data)\n",
    "            \n",
    "            # Create child nodes \n",
    "            right_child_node = decisionTreeNode(right_split)\n",
    "            left_child_node = decisionTreeNode(left_split)\n",
    "            parent_node.children = [left_child_node, right_child_node]\n",
    "            \n",
    "            # Check Purity of Child Nodes\n",
    "            right_purity = right_child_node.isPure(label_index)\n",
    "            left_purity = left_child_node.isPure(label_index)\n",
    "            if right_purity == False: \n",
    "                # Add to impure nodes list\n",
    "                self.impure_leaf_nodes.append(right_child_node)\n",
    "            else: \n",
    "                # Add prediction label to leaf node\n",
    "                label_index = (right_child_node.data.shape[1] - 1)\n",
    "                right_child_node.pure = True\n",
    "                right_child_node.predicted_label = right_child_node.data[0,label_index]\n",
    "            if left_purity == False: \n",
    "                # Add to impure nodes list\n",
    "                self.impure_leaf_nodes.append(left_child_node)\n",
    "            else: \n",
    "                # Add prediction label to leaf node\n",
    "                label_index = (left_child_node.data.shape[1] - 1)\n",
    "                left_child_node.pure = True\n",
    "                left_child_node.predicted_label = left_child_node.data[0,label_index]\n",
    "                \n",
    "    def predictHelper(self, data_point, currNode):\n",
    "        # Base Case\n",
    "        if currNode.pure: \n",
    "            label = currNode.predicted_label\n",
    "            return label\n",
    "        # recurse to the left\n",
    "        if data_point[currNode.feature] < currNode.threshold: \n",
    "            leftNode = currNode.children[0]\n",
    "            return self.predictHelper(data_point, leftNode)\n",
    "        # recurse to the right\n",
    "        elif data_point[currNode.feature] > currNode.threshold: \n",
    "            rightNode = currNode.children[1]\n",
    "            return self.predictHelper(data_point,rightNode)\n",
    "    \n",
    "    def predict(self, data, root): \n",
    "        predictions = []\n",
    "        for i in range(data.shape[0]):\n",
    "            predictions.append(self.predictHelper(data[i,:data.shape[0]-1],self.root))\n",
    "        # Return Prediction Error\n",
    "        return calc_error(predictions ,data[:,data.shape[1]-1]) \n",
    "    \n",
    "    # Define Pruning Algorithm\n",
    "    def tree_pruning(self, root, validation_set):\n",
    "        # Breadth First Search\n",
    "        queue = []\n",
    "        queue.append(root)\n",
    "        root.visited = True\n",
    "        counter = 0\n",
    "        # While there are still elements in the queue\n",
    "        while len(queue) != 0: \n",
    "            node = queue.pop(0)\n",
    "            T_error = self.predict(validation_set, root)\n",
    "            # To Obtain T', just set node.pure equal True\n",
    "            node.pure = True\n",
    "            # Set label equal to majority label\n",
    "            node.predicted_label = statistics.mode(node.data[:,node.data.shape[1]-1])\n",
    "            T_prime_error = self.predict(validation_set, root)\n",
    "            if T_error <= T_prime_error: \n",
    "                # Keep tree the same\n",
    "                node.pure = False\n",
    "                node.predicted_label = None\n",
    "            else: \n",
    "                print(\"Level \" + str(counter + 1) + \" Error: \", T_prime_error)\n",
    "                node.children = [] # delete subtree rooted at node\n",
    "                counter += 1\n",
    "            if counter >= 2: \n",
    "                # Stop tree pruning\n",
    "                break\n",
    "            # Replace Subtree Rooted at Node W/ Majority Label and compare accuracies\n",
    "            for i in range(len(node.children)): \n",
    "                # Only add node to queue if it hasn't been visited\n",
    "                if node.children[i].visited == False: \n",
    "                    queue.append(node.children[i])\n",
    "                    node.visited = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Feature and Threshold:  (1, 0.5)  Conditional Entropy:  0.2727917864120626\n"
     ]
    }
   ],
   "source": [
    "# Example from class\n",
    "data = np.array([[0,0,1],[1,0,1],[1,1,0],[2,1,0],[2,0,0],[1,2,0],[2,2,0]])\n",
    "split_rule, info = find_decision_rule(data)\n",
    "print(\"Optimal Feature and Threshold: \", split_rule,' Conditional Entropy: ', info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Sanity Check, Features are Zero Indexed\n",
    "dt_training = decisionTree(training_data)\n",
    "dt_training.train(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Threshold Value:  0.5 Root Feature Number  5 Number of Samples:  2000\n",
      "Root Left Child Threshold :  415000.0 Root Left Child Feature Number:  1 Number of Samples:  1319\n",
      "Root Right Child Threshold :  1.5 Root Left Child Feature Number:  5 Number of Samples:  681\n",
      "Root Left Left Threshold:  2506.5 Root Left Left Feature Number:  17 Number of Samples:  1284\n",
      "Root Left Right Threshold:  208.0 Root Left Right Feature Number:  21 Number of Samples:  35\n",
      "Root Right Left Threshold:  584.5 Root Right Left Feature Number:  20 Number of Samples:  292\n",
      "Root Right Right Threshold:  2006.0 Root Right Right Feature Number:  21 Number of Samples:  389\n"
     ]
    }
   ],
   "source": [
    "root = dt_training.root\n",
    "root_left = root.children[0]\n",
    "root_right = root.children[1]\n",
    "left_left = root_left.children[0]\n",
    "left_right = root_left.children[1]\n",
    "right_left = root_right.children[0]\n",
    "right_right = root_right.children[1]\n",
    "\n",
    "print(\"Root Threshold Value: \",root.threshold,\"Root Feature Number \", (root.feature + 1), \"Number of Samples: \", root.data.shape[0])\n",
    "print(\"Root Left Child Threshold : \", root_left.threshold, \"Root Left Child Feature Number: \", (root_left.feature + 1), \"Number of Samples: \",root_left.data.shape[0])\n",
    "print(\"Root Right Child Threshold : \", root_right.threshold, \"Root Left Child Feature Number: \",(root_right.feature + 1), \"Number of Samples: \",root_right.data.shape[0])\n",
    "print(\"Root Left Left Threshold: \", left_left.threshold, \"Root Left Left Feature Number: \", (left_left.feature+1),\"Number of Samples: \", left_left.data.shape[0])\n",
    "print(\"Root Left Right Threshold: \", left_right.threshold, \"Root Left Right Feature Number: \", (left_right.feature+1),\"Number of Samples: \", left_right.data.shape[0])\n",
    "print(\"Root Right Left Threshold: \", right_left.threshold, \"Root Right Left Feature Number: \", (right_left.feature+1),\"Number of Samples: \", right_left.data.shape[0])\n",
    "print(\"Root Right Right Threshold: \", right_right.threshold, \"Root Right Right Feature Number: \", (right_right.feature+1),\"Number of Samples: \", right_right.data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Error:  0.0\n",
      "Testing Error (Without Pruning) : 0.173\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Error: \", dt_training.predict(training_data ,dt_training.root))\n",
    "print(\"Testing Error (Without Pruning) :\", dt_training.predict(test_data ,dt_training.root))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Level 1 Error:  0.121\n",
      "Level 2 Error:  0.107\n"
     ]
    }
   ],
   "source": [
    "dt_training.tree_pruning(dt_training.root, validation_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
