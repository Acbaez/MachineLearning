{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 23)\n"
     ]
    }
   ],
   "source": [
    "# Read in datafiles\n",
    "training_data = np.loadtxt('./data/pa2train.txt')\n",
    "print(training_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 23)\n"
     ]
    }
   ],
   "source": [
    "# Read in datafiles\n",
    "validation_data = np.loadtxt('./data/pa2validation.txt')\n",
    "print(validation_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 23)\n"
     ]
    }
   ],
   "source": [
    "# Read in datafiles\n",
    "test_data = np.loadtxt('./data/pa2test.txt')\n",
    "print(test_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating info gain\n",
    "def info_gain(subset, feature, threshold, label):\n",
    "    \n",
    "    num_samples = len(subset)\n",
    "    # Partition Subset into two sets v1, v2\n",
    "    v1, v2 = [x for x in subset if x[feature] < threshold], [x for x in subset if x[feature] >= threshold]\n",
    "    v1_samples, v2_samples = len(v1), len(v2)\n",
    "    \n",
    "    # Find distribution of labels for both partitions\n",
    "    v1_default, v2_default = len([x for x in v1 if x[label] == 1]), len([x for x in v2 if x[label] == 0])\n",
    "    v1_no_default, v2_no_default = (v1_samples - v1_default), (v2_samples - v2_default)\n",
    "    \n",
    "    # Compare distribution of labels in both subsets, P(Z=z)\n",
    "    p_yes_threshold =  (v2_samples/num_samples)\n",
    "    p_no_threshold = (1 - p_yes_threshold)\n",
    "    \n",
    "    # Calculate conditional entropy for sboth partitions H(X|Z=z)\n",
    "    if v1_default == 0 or v1_no_default == 0: \n",
    "        if v1_samples == 0: \n",
    "            cond_entropy_no = 0\n",
    "        elif v1_default == 0: \n",
    "            cond_entropy_no = ((v1_no_default/len(v1))*math.log(v1_no_default/len(v1)))\n",
    "        else: \n",
    "            cond_entropy_no = ((v1_default/len(v1))*math.log(v1_default/len(v1)))\n",
    "    else:\n",
    "        cond_entropy_no = -(((v1_no_default/len(v1))*math.log(v1_no_default/len(v1)))+((v1_default/len(v1))*math.log(v1_default/len(v1))))\n",
    "        # Calculate conditional entropy for sboth partitions H(X|Z=z)\n",
    "    \n",
    "    if v2_default == 0 or v2_no_default == 0: \n",
    "        if v2_samples == 0: \n",
    "            cond_entropy_yes = 0\n",
    "        elif v2_default == 0: \n",
    "            cond_entropy_yes = ((v2_no_default/len(v2))*math.log(v2_no_default/len(v2)))\n",
    "        else: \n",
    "            cond_entropy_yes = ((v2_default/len(v2))*math.log(v2_default/len(v2)))\n",
    "    else:\n",
    "        cond_entropy_yes = -(((v2_default/len(v2))*math.log(v2_default/len(v2)))+((v2_no_default/len(v2))*math.log(v2_no_default/len(v2)))) \n",
    "    \n",
    "    # return overall conditional entropy H(X|Z)\n",
    "    return (cond_entropy_yes*(len(v2)/num_samples) + cond_entropy_no*(len(v1)/num_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Function for Obtaining threshold values\n",
    "def get_thresholds(feature_vals):\n",
    "    thresholds = []\n",
    "    features = sorted(set(feature_vals))\n",
    "    for i in range(1,len(features)): \n",
    "        thresholds.append((feature_vals[i-1]+feature_vals[i])/2)\n",
    "    return thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_threshold(feature, threshold, data): \n",
    "    return [x for x in data if x[feature] < threshold], [x for x in data if x[feature] >= threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_decision_rule(training_samples): \n",
    "    # Pick feature, threshold pair that maxes info gain \n",
    "    split_rule = {}\n",
    "    for i in range(training_samples.shape[1]-1):\n",
    "        feature_dict = {}\n",
    "        # Obtain thresholds\n",
    "        thresholds = get_thresholds(training_samples[:,i])\n",
    "        for threshold in thresholds: \n",
    "            # Calculate info gain for threshold-feature pair\n",
    "            ig = info_gain(training_samples, i, threshold, training_samples.shape[1]-1)\n",
    "            # Append to dictionary \n",
    "            feature_dict[ig] = (i, threshold)\n",
    "            # Use feature-threshold pair with max info gain\n",
    "            max_ig = sorted(feature_dict.keys())[0]\n",
    "            feature_threshold = feature_dict[max_ig]\n",
    "            split_rule[max_ig] = feature_threshold      \n",
    "    # Find final split rule\n",
    "    split = split_rule[sorted(split_rule.keys())[0]]\n",
    "    return (split[0],split[1]), sorted(split_rule.keys())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Feature and Threshold:  (1, 0.5)  Conditional Entropy:  0.2727917864120626\n"
     ]
    }
   ],
   "source": [
    "# Test functions\n",
    "test_data = np.array([[0,0,1],[1,0,1],[1,1,0],[2,1,0],[2,0,0],[1,2,0],[2,2,0]])\n",
    "\n",
    "split_rule, info_gain = find_decision_rule(test_data)\n",
    "print(\"Optimal Feature and Threshold: \", split_rule,' Conditional Entropy: ', info_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Decision Tree Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Decision Tree\n",
    "class decisionTree: \n",
    "    # Define Decision Tree Node Class\n",
    "    class decisionTreeNode: \n",
    "        \n",
    "        # Define constructor\n",
    "        def __init__(self, data):\n",
    "            self.children = []\n",
    "            self.pure = False\n",
    "            self.feature = 0\n",
    "            self.threshold = 0\n",
    "            self.data = data\n",
    "            self.predicted_label = None\n",
    "            self.entropy = float(0.0)\n",
    "        \n",
    "        def isPure(self, label): \n",
    "            isPure = False\n",
    "            labels = [x[label] for x in self.data]\n",
    "            # Check if labels for node are pure\n",
    "            if len(set(labels)) == 1: \n",
    "                isPure = True\n",
    "            return isPure\n",
    "    \n",
    "    # Define Decision Tree Constructor\n",
    "    def __init__(self, training_data):\n",
    "        self.root = decisionTreeNode(training_data)\n",
    "        self.impure_leaf_nodes = [self.root]\n",
    "    \n",
    "    \n",
    "    # Function for building decision tree\n",
    "    def trainDecisionTree(self, training_samples): \n",
    "        # Continue Algorithm until all leaf nodes are pure\n",
    "        while len(self.impure_leaf_nodes) != 0: \n",
    "            # Pick an impure node V and remove from list\n",
    "            parent_node = self.impure_leaf_nodes[-1]\n",
    "            self.impure_leaf_nodes.pop(-1)\n",
    "            split_rules = {}\n",
    "            \n",
    "            # Find decision rule for parent node\n",
    "            split_rule, cond_entropy = find_decision_rule(parent_node.data)\n",
    "            parent_node.feature = split_rule[0]\n",
    "            parent_node.threshold = split_rule[1]\n",
    "            \n",
    "            # Define subsets based on splitting rule\n",
    "            right_split, left_split = split_threshold(parent_node.feature, parent_node.threshold, training_samples)\n",
    "            # Create child nodes \n",
    "            right_child_node = decisionTreeNode(right_split)\n",
    "            left_child_node = decisionTreeNode(left_split)\n",
    "            parent_node.children = [left_child_node, right_child_node]\n",
    "            \n",
    "            # Check Purity of Child Nodes\n",
    "            right_purity = right_child_node.isPure(label)\n",
    "            left_purity = left_child_node.isPure(label)\n",
    "            if right_purity == False: \n",
    "                # Add to impure nodes list\n",
    "                self.impure_leaf_nodes.append(right_child_node)\n",
    "            else: \n",
    "                # Add prediction label to leaf node\n",
    "                label_index = right_child_node.data.shape[1]\n",
    "                right_child_node.predicted_label = right_child_node.data[0,label_index]\n",
    "            if left_purity == False: \n",
    "                # Add to impure nodes list\n",
    "                self.impure_leaf_nodes.append(left_child_node)\n",
    "            else: \n",
    "                # Add prediction label to leaf node\n",
    "                label_index = left_child_node.data.shape[1]\n",
    "                left_child_node.predicted_label = left_child_node.data[0,label_index]\n",
    "        \n",
    "        # Return root of contructed decision tree\n",
    "        return self.root\n",
    "    \n",
    "    # Define Pruning Algorithm\n",
    "    def tree_pruning(self):\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
