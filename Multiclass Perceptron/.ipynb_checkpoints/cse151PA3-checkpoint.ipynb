{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 3: Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in Datasets\n",
    "\n",
    "f = open('./data/pa3dictionary.txt', 'r')\n",
    "words = f.readlines()\n",
    "f.close()\n",
    "\n",
    "training_data = pd.read_csv('./data/pa3train.txt', sep=\" \", header=None)\n",
    "testing_data = pd.read_csv('./data/pa3test.txt', sep=\" \", header=None)\n",
    "\n",
    "label_index = training_data.shape[1] - 1\n",
    "training_data_q1 = training_data.loc[training_data[label_index] <= 2]\n",
    "testing_data_q1 = testing_data.loc[testing_data[label_index] <= 2]\n",
    "\n",
    "# Map all 2 and 1 values to 1, -1\n",
    "\n",
    "for i, row in training_data_q1.iterrows(): \n",
    "    if row[label_index] == 1: \n",
    "        row[label_index] = -1\n",
    "    else: \n",
    "        row[label_index] = 1\n",
    "\n",
    "# Map all 2 and 1 values to 1, -1\n",
    "for i, row in testing_data_q1.iterrows(): \n",
    "    if row[label_index] == 1: \n",
    "        row[label_index] = -1\n",
    "    else: \n",
    "        row[label_index] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error(Y_pred, Y_label): \n",
    "    # Calculate Error Rate for Predicted Labels\n",
    "    error = [0 for x,y in zip(Y_pred,Y_label) if x != y]\n",
    "    error_rate = len(error)/len(Y_pred)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.w = np.empty((0,0))\n",
    "        self.w_list = []\n",
    "        self.c_list = []\n",
    "    \n",
    "    # Make Predictions\n",
    "    def predict(self, test_data, model):\n",
    "        predictions = []\n",
    "        # Error Checking\n",
    "        if model not in ['normal','voted','averaged']:\n",
    "            print(\"ERROR\")\n",
    "            return []\n",
    "        for i,row in test_data.iterrows():\n",
    "            sample_point = row[:test_data.shape[1]-1]\n",
    "            if model == 'normal': \n",
    "                prediction = np.sign(np.dot(self.w, sample_point))\n",
    "                if prediction == 0: \n",
    "                    prediction = rand.choice([1,-1])\n",
    "                predictions.append(prediction)\n",
    "            elif model == 'voted': \n",
    "                prediction = np.sign(sum(c*(np.sign(np.dot(w,sample_point))) for c, w in zip(self.c_list, self.w_list)))\n",
    "                if prediction == 0: \n",
    "                    prediction = rand.choice([1,-1])\n",
    "                predictions.append(prediction)\n",
    "            elif model == 'averaged': \n",
    "                prediction = np.sign(np.dot((sum(c*w for c, w in zip(self.c_list, self.w_list))),sample_point))\n",
    "                if prediction == 0: \n",
    "                    prediction = rand.choice([1,-1])\n",
    "                predictions.append(prediction)\n",
    "        return predictions\n",
    "    \n",
    "    # Train Classifier\n",
    "    def train_multiclass(self, data, positive_label, num_passes): \n",
    "        w_list = []\n",
    "        c_list = []\n",
    "        c = 0\n",
    "        w = np.zeros((data.shape[1]-1,))\n",
    "        for p in range(num_passes): \n",
    "            for i, row in data.iterrows(): \n",
    "                X = row[:data.shape[1]-1]\n",
    "                Y = row[data.shape[1]-1]\n",
    "                if Y == positive_label: \n",
    "                    Y = 1\n",
    "                else: \n",
    "                    Y = -1\n",
    "                if Y*(np.dot(w,np.transpose(X))) <= 0: \n",
    "                    # Adjust decision boundary, otherwise keep it \n",
    "                    w = np.add(w,Y*X)\n",
    "                    w_list.append(w)\n",
    "                    c_list.append(c)\n",
    "                    c = 1\n",
    "                else: \n",
    "                    c += 1\n",
    "        c_list.append(c)\n",
    "        self.w_list = w_list\n",
    "        self.w = w_list[-1]  \n",
    "        self.c_list = c_list\n",
    "    \n",
    "    def train(self, data, num_passes): \n",
    "        w_list = []\n",
    "        c_list = []\n",
    "        c = 0\n",
    "        w = np.zeros((data.shape[1]-1,))\n",
    "        for p in range(num_passes): \n",
    "            for i, row in data.iterrows(): \n",
    "                X = row[:data.shape[1]-1]\n",
    "                Y = row[data.shape[1]-1]\n",
    "                if Y*(np.dot(w,np.transpose(X))) <= 0: \n",
    "                    # Adjust decision boundary, otherwise keep it \n",
    "                    w = np.add(w,Y*X)\n",
    "                    w_list.append(w)\n",
    "                    c_list.append(c)\n",
    "                    c = 1\n",
    "                else: \n",
    "                    c += 1\n",
    "        c_list.append(c)\n",
    "        self.w_list = w_list\n",
    "        self.w = w_list[-1]  \n",
    "        self.c_list = c_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf1 = perceptron()\n",
    "clf1.train(training_data_q1, 2)\n",
    "y_pred_normal = clf1.predict(training_data_q1, 'normal')\n",
    "y_pred_avg = clf1.predict(training_data_q1, 'averaged')\n",
    "y_pred_voted = clf1.predict(training_data_q1, 'voted')\n",
    "\n",
    "y_pred_normal_test = clf1.predict(testing_data_q1, 'normal')\n",
    "y_pred_avg_test = clf1.predict(testing_data_q1, 'averaged')\n",
    "y_pred_voted_test = clf1.predict(testing_data_q1, 'voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ERROR RATES FOR TWO PASSES\n",
      "Error Rate Normal :  0.03669724770642202\n",
      "Error Rate Voted:  0.03577981651376147\n",
      "Error Rate Average:  0.05504587155963303\n",
      "TESTING ERROR RATES FOR TWO PASSES\n",
      "Error Rate Normal :  0.058355437665782495\n",
      "Error Rate Voted:  0.05305039787798409\n",
      "Error Rate Average:  0.07957559681697612\n"
     ]
    }
   ],
   "source": [
    "y_true = training_data_q1[label_index]\n",
    "y_true_test = testing_data_q1[label_index]\n",
    "print(\"TRAINING ERROR RATES FOR TWO PASSES\")\n",
    "print(\"Error Rate Normal : \",calc_error(y_pred_normal, y_true))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted, y_true))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg, y_true))\n",
    "\n",
    "print(\"TESTING ERROR RATES FOR TWO PASSES\")\n",
    "print(\"Error Rate Normal : \",calc_error(y_pred_normal_test, y_true_test))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted_test, y_true_test))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg_test, y_true_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = perceptron()\n",
    "clf2.train(training_data_q1, 3)\n",
    "y_pred_normal = clf2.predict(training_data_q1, 'normal')\n",
    "y_pred_avg = clf2.predict(training_data_q1, 'averaged')\n",
    "y_pred_voted = clf2.predict(training_data_q1, 'voted')\n",
    "\n",
    "y_pred_normal_test = clf2.predict(testing_data_q1, 'normal')\n",
    "y_pred_avg_test = clf2.predict(testing_data_q1, 'averaged')\n",
    "y_pred_voted_test = clf2.predict(testing_data_q1, 'voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR RATES FOR THREE PASSES\n",
      "Error Rate Normal:  0.01926605504587156\n",
      "Error Rate Voted:  0.03211009174311927\n",
      "Error Rate Average:  0.03577981651376147\n",
      "TESTING ERROR RATES FOR THREE PASSES\n",
      "Error Rate Normal :  0.04509283819628647\n",
      "Error Rate Voted:  0.04774535809018567\n",
      "Error Rate Average:  0.0610079575596817\n"
     ]
    }
   ],
   "source": [
    "y_true = training_data_q1[label_index]\n",
    "print(\"ERROR RATES FOR THREE PASSES\")\n",
    "print(\"Error Rate Normal: \",calc_error(y_pred_normal, y_true))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted, y_true))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg, y_true))\n",
    "\n",
    "print(\"TESTING ERROR RATES FOR THREE PASSES\")\n",
    "print(\"Error Rate Normal : \",calc_error(y_pred_normal_test, y_true_test))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted_test, y_true_test))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg_test, y_true_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = perceptron()\n",
    "clf3.train(training_data_q1, 4)\n",
    "y_pred_normal = clf3.predict(training_data_q1, 'normal')\n",
    "y_pred_avg = clf3.predict(training_data_q1, 'averaged')\n",
    "y_pred_voted = clf3.predict(training_data_q1, 'voted')\n",
    "\n",
    "y_pred_normal_test = clf3.predict(testing_data_q1, 'normal')\n",
    "y_pred_avg_test = clf3.predict(testing_data_q1, 'averaged')\n",
    "y_pred_voted_test = clf3.predict(testing_data_q1, 'voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR RATES FOR FOUR PASSES\n",
      "Error Rate Normal:  0.01743119266055046\n",
      "Error Rate Voted:  0.02110091743119266\n",
      "Error Rate Average:  0.03211009174311927\n",
      "TESTING ERROR RATES FOR FOUR PASSES\n",
      "Error Rate Normal :  0.04774535809018567\n",
      "Error Rate Voted:  0.04509283819628647\n",
      "Error Rate Average:  0.05305039787798409\n"
     ]
    }
   ],
   "source": [
    "y_true = training_data_q1[label_index]\n",
    "print(\"ERROR RATES FOR FOUR PASSES\")\n",
    "print(\"Error Rate Normal: \",calc_error(y_pred_normal, y_true))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted, y_true))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg, y_true))\n",
    "\n",
    "print(\"TESTING ERROR RATES FOR FOUR PASSES\")\n",
    "print(\"Error Rate Normal : \",calc_error(y_pred_normal_test, y_true_test))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted_test, y_true_test))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg_test, y_true_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top (Strong Positive Predictors): \n",
      " 816    393\n",
      "817    469\n",
      "818     78\n",
      "dtype: int64\n",
      "Bottom (Strong Negative Predictors): \n",
      " 0    438\n",
      "1    466\n",
      "2    203\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "w_avg = (sum(c*w for c, w in zip(clf2.c_list, clf2.w_list)))\n",
    "sorted_w = np.argsort(w_avg)\n",
    "print(\"Top (Strong Positive Predictors): \\n\", sorted_w[-3:])\n",
    "print(\"Bottom (Strong Negative Predictors): \\n\", sorted_w[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game  team  he \n",
      "file  program  line \n"
     ]
    }
   ],
   "source": [
    "print(words[393].replace('\\n',''),words[469].replace('\\n',''),words[78].replace('\\n',''))\n",
    "print(words[438].replace('\\n',''),words[466].replace('\\n',''),words[203].replace('\\n',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class one_v_all:\n",
    "    def __init__(self): \n",
    "        self.classifiers = []\n",
    "    \n",
    "    def train(self, num_classes, training_data):\n",
    "        classifiers = []\n",
    "        for i in range(num_classes): \n",
    "            clf = perceptron()\n",
    "            clf.train_multiclass(training_data, i+1, 1)\n",
    "            classifiers.append(clf)\n",
    "        self.classifiers = classifiers\n",
    "    \n",
    "    def predictSample(self, training_sample):\n",
    "        predictions = []\n",
    "        for clf in self.classifiers: \n",
    "            prediction = clf.predict(training_sample,'normal')\n",
    "            predictions.append(prediction[0])\n",
    "        if predictions.count(1) != 1: \n",
    "            # Return \"Don't Know\"\n",
    "            return 0\n",
    "        else: \n",
    "            # Predict whatever i classifier correctly predicted for\n",
    "            return (predictions.index(1) + 1)\n",
    "    def predict(self, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = one_v_all()\n",
    "clf.train(6, training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.0, -1.0, -1.0, -1.0, -1.0, 1.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.predictSample(testing_data.loc[[4]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
