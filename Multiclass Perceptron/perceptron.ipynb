{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 3: Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in Datasets\n",
    "\n",
    "f = open('./data/pa3dictionary.txt', 'r')\n",
    "words = f.readlines()\n",
    "f.close()\n",
    "\n",
    "training_data = pd.read_csv('./data/pa3train.txt', sep=\" \", header=None)\n",
    "testing_data = pd.read_csv('./data/pa3test.txt', sep=\" \", header=None)\n",
    "\n",
    "label_index = training_data.shape[1] - 1\n",
    "training_data_q1 = training_data.loc[training_data[label_index] <= 2]\n",
    "testing_data_q1 = testing_data.loc[testing_data[label_index] <= 2]\n",
    "\n",
    "# Map all 2 and 1 values to 1, -1\n",
    "\n",
    "for i, row in training_data_q1.iterrows(): \n",
    "    if row[label_index] == 1: \n",
    "        row[label_index] = -1\n",
    "    else: \n",
    "        row[label_index] = 1\n",
    "\n",
    "# Map all 2 and 1 values to 1, -1\n",
    "for i, row in testing_data_q1.iterrows(): \n",
    "    if row[label_index] == 1: \n",
    "        row[label_index] = -1\n",
    "    else: \n",
    "        row[label_index] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error(Y_pred, Y_label): \n",
    "    # Calculate Error Rate for Predicted Labels\n",
    "    error = [0 for x,y in zip(Y_pred,Y_label) if x != y]\n",
    "    error_rate = len(error)/len(Y_pred)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.w = np.empty((0,0))\n",
    "        self.w_list = []\n",
    "        self.c_list = []\n",
    "    \n",
    "    # Make Predictions\n",
    "    def predict(self, test_data, model):\n",
    "        predictions = []\n",
    "        # Error Checking\n",
    "        if model not in ['normal','voted','averaged']:\n",
    "            print(\"ERROR\")\n",
    "            return []\n",
    "        for i,row in test_data.iterrows():\n",
    "            sample_point = row[:test_data.shape[1]-1]\n",
    "            if model == 'normal': \n",
    "                prediction = np.sign(np.dot(self.w, sample_point))\n",
    "                if prediction == 0: \n",
    "                    prediction = rand.choice([1,-1])\n",
    "                predictions.append(prediction)\n",
    "            elif model == 'voted': \n",
    "                prediction = np.sign(sum(c*(np.sign(np.dot(w,sample_point))) for c, w in zip(self.c_list, self.w_list)))\n",
    "                if prediction == 0: \n",
    "                    prediction = rand.choice([1,-1])\n",
    "                predictions.append(prediction)\n",
    "            elif model == 'averaged': \n",
    "                prediction = np.sign(np.dot((sum(c*w for c, w in zip(self.c_list, self.w_list))),sample_point))\n",
    "                if prediction == 0: \n",
    "                    prediction = rand.choice([1,-1])\n",
    "                predictions.append(prediction)\n",
    "        return predictions\n",
    "    \n",
    "    \n",
    "    # Train Classifier\n",
    "    def train_multiclass(self, data, positive_label, num_passes): \n",
    "        w_list = []\n",
    "        c_list = []\n",
    "        c = 0\n",
    "        w = np.zeros((data.shape[1]-1,))\n",
    "        for p in range(num_passes): \n",
    "            for i, row in data.iterrows(): \n",
    "                X = row[:data.shape[1]-1]\n",
    "                Y = row[data.shape[1]-1]\n",
    "                if Y == positive_label: \n",
    "                    Y = 1\n",
    "                else: \n",
    "                    Y = -1\n",
    "                if Y*(np.dot(w,np.transpose(X))) <= 0: \n",
    "                    # Adjust decision boundary, otherwise keep it \n",
    "                    w = np.add(w,Y*X)\n",
    "                    w_list.append(w)\n",
    "                    c_list.append(c)\n",
    "                    c = 1\n",
    "                else: \n",
    "                    c += 1\n",
    "        c_list.append(c)\n",
    "        self.w_list = w_list\n",
    "        self.w = w_list[-1]  \n",
    "        self.c_list = c_list\n",
    "    \n",
    "    def train(self, data, num_passes): \n",
    "        w_list = []\n",
    "        c_list = []\n",
    "        c = 0\n",
    "        w = np.zeros((data.shape[1]-1,))\n",
    "        for p in range(num_passes): \n",
    "            for i, row in data.iterrows(): \n",
    "                X = row[:data.shape[1]-1]\n",
    "                Y = row[data.shape[1]-1]\n",
    "                if Y*(np.dot(w,np.transpose(X))) <= 0: \n",
    "                    # Adjust decision boundary, otherwise keep it \n",
    "                    w = np.add(w,Y*X)\n",
    "                    w_list.append(w)\n",
    "                    c_list.append(c)\n",
    "                    c = 1\n",
    "                else: \n",
    "                    c += 1\n",
    "        c_list.append(c)\n",
    "        self.w_list = w_list\n",
    "        self.w = w_list[-1]  \n",
    "        self.c_list = c_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf1 = perceptron()\n",
    "clf1.train(training_data_q1, 2)\n",
    "y_pred_normal = clf1.predict(training_data_q1, 'normal')\n",
    "y_pred_avg = clf1.predict(training_data_q1, 'averaged')\n",
    "y_pred_voted = clf1.predict(training_data_q1, 'voted')\n",
    "\n",
    "y_pred_normal_test = clf1.predict(testing_data_q1, 'normal')\n",
    "y_pred_avg_test = clf1.predict(testing_data_q1, 'averaged')\n",
    "y_pred_voted_test = clf1.predict(testing_data_q1, 'voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING ERROR RATES FOR TWO PASSES\n",
      "Error Rate Normal :  0.03669724770642202\n",
      "Error Rate Voted:  0.03761467889908257\n",
      "Error Rate Average:  0.05412844036697248\n",
      "TESTING ERROR RATES FOR TWO PASSES\n",
      "Error Rate Normal :  0.0610079575596817\n",
      "Error Rate Voted:  0.05305039787798409\n",
      "Error Rate Average:  0.07957559681697612\n"
     ]
    }
   ],
   "source": [
    "y_true = training_data_q1[label_index]\n",
    "y_true_test = testing_data_q1[label_index]\n",
    "print(\"TRAINING ERROR RATES FOR TWO PASSES\")\n",
    "print(\"Error Rate Normal : \",calc_error(y_pred_normal, y_true))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted, y_true))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg, y_true))\n",
    "\n",
    "print(\"TESTING ERROR RATES FOR TWO PASSES\")\n",
    "print(\"Error Rate Normal : \",calc_error(y_pred_normal_test, y_true_test))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted_test, y_true_test))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg_test, y_true_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = perceptron()\n",
    "clf2.train(training_data_q1, 3)\n",
    "y_pred_normal = clf2.predict(training_data_q1, 'normal')\n",
    "y_pred_avg = clf2.predict(training_data_q1, 'averaged')\n",
    "y_pred_voted = clf2.predict(training_data_q1, 'voted')\n",
    "\n",
    "y_pred_normal_test = clf2.predict(testing_data_q1, 'normal')\n",
    "y_pred_avg_test = clf2.predict(testing_data_q1, 'averaged')\n",
    "y_pred_voted_test = clf2.predict(testing_data_q1, 'voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR RATES FOR THREE PASSES\n",
      "Error Rate Normal:  0.01926605504587156\n",
      "Error Rate Voted:  0.030275229357798167\n",
      "Error Rate Average:  0.03669724770642202\n",
      "TESTING ERROR RATES FOR THREE PASSES\n",
      "Error Rate Normal :  0.04509283819628647\n",
      "Error Rate Voted:  0.04774535809018567\n",
      "Error Rate Average:  0.0610079575596817\n"
     ]
    }
   ],
   "source": [
    "y_true = training_data_q1[label_index]\n",
    "print(\"ERROR RATES FOR THREE PASSES\")\n",
    "print(\"Error Rate Normal: \",calc_error(y_pred_normal, y_true))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted, y_true))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg, y_true))\n",
    "\n",
    "print(\"TESTING ERROR RATES FOR THREE PASSES\")\n",
    "print(\"Error Rate Normal : \",calc_error(y_pred_normal_test, y_true_test))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted_test, y_true_test))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg_test, y_true_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf3 = perceptron()\n",
    "clf3.train(training_data_q1, 4)\n",
    "y_pred_normal = clf3.predict(training_data_q1, 'normal')\n",
    "y_pred_avg = clf3.predict(training_data_q1, 'averaged')\n",
    "y_pred_voted = clf3.predict(training_data_q1, 'voted')\n",
    "\n",
    "y_pred_normal_test = clf3.predict(testing_data_q1, 'normal')\n",
    "y_pred_avg_test = clf3.predict(testing_data_q1, 'averaged')\n",
    "y_pred_voted_test = clf3.predict(testing_data_q1, 'voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR RATES FOR FOUR PASSES\n",
      "Error Rate Normal:  0.01651376146788991\n",
      "Error Rate Voted:  0.023853211009174313\n",
      "Error Rate Average:  0.03211009174311927\n",
      "TESTING ERROR RATES FOR FOUR PASSES\n",
      "Error Rate Normal :  0.04509283819628647\n",
      "Error Rate Voted:  0.04509283819628647\n",
      "Error Rate Average:  0.05305039787798409\n"
     ]
    }
   ],
   "source": [
    "y_true = training_data_q1[label_index]\n",
    "print(\"ERROR RATES FOR FOUR PASSES\")\n",
    "print(\"Error Rate Normal: \",calc_error(y_pred_normal, y_true))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted, y_true))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg, y_true))\n",
    "\n",
    "print(\"TESTING ERROR RATES FOR FOUR PASSES\")\n",
    "print(\"Error Rate Normal : \",calc_error(y_pred_normal_test, y_true_test))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted_test, y_true_test))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg_test, y_true_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top (Strong Positive Predictors): \n",
      " 816    393\n",
      "817    469\n",
      "818     78\n",
      "dtype: int64\n",
      "Bottom (Strong Negative Predictors): \n",
      " 0    438\n",
      "1    466\n",
      "2    203\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "w_avg = (sum(c*w for c, w in zip(clf2.c_list, clf2.w_list)))\n",
    "sorted_w = np.argsort(w_avg)\n",
    "print(\"Top (Strong Positive Predictors): \\n\", sorted_w[-3:])\n",
    "print(\"Bottom (Strong Negative Predictors): \\n\", sorted_w[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "game  team  he \n",
      "file  program  line \n"
     ]
    }
   ],
   "source": [
    "print(words[393].replace('\\n',''),words[469].replace('\\n',''),words[78].replace('\\n',''))\n",
    "print(words[438].replace('\\n',''),words[466].replace('\\n',''),words[203].replace('\\n',''))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class one_v_all:\n",
    "    def __init__(self): \n",
    "        self.classifiers = []\n",
    "    \n",
    "    def train(self, num_classes, training_data):\n",
    "        classifiers = []\n",
    "        for i in range(num_classes): \n",
    "            clf = perceptron()\n",
    "            clf.train_multiclass(training_data, i+1, 1)\n",
    "            classifiers.append(clf)\n",
    "        self.classifiers = classifiers\n",
    "    \n",
    "    def predictSample(self, test_sample):\n",
    "        predictions = []\n",
    "        for clf in self.classifiers: \n",
    "            prediction = clf.predict(test_sample,'normal')\n",
    "            predictions.append(prediction[0])\n",
    "        if predictions.count(1) != 1: \n",
    "            # Return \"Don't Know\"\n",
    "            return 0\n",
    "        else: \n",
    "            # Predict whatever i classifier correctly predicted for\n",
    "            return (predictions.index(1) + 1)\n",
    "        \n",
    "    def predict(self, test_data): \n",
    "        predictions = []\n",
    "        for i in range(test_data.shape[0]): \n",
    "            prediction = self.predictSample(test_data.iloc[[i]])\n",
    "            predictions.append(prediction)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(y_pred, y_true): \n",
    "    # dim = number of labels in prediction task\n",
    "    rows = len(set(y_pred))\n",
    "    cols = len(set(y_true))\n",
    "    C = np.zeros((rows, cols))\n",
    "    # Find number of each class label in test data\n",
    "    num_label = [0]*cols\n",
    "    for i in range(len(num_label)): \n",
    "        label = y_true.count(i+1)\n",
    "        num_label[i] = label\n",
    "    for x, y in zip(y_pred, y_true): \n",
    "        C[x,y-1] += 1\n",
    "    for i in range(rows): \n",
    "        for j in range(cols): \n",
    "            C[i,j] /= num_label[j]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clf = one_v_all()\n",
    "clf.train(6, training_data)\n",
    "y_pred = clf.predict(training_data)\n",
    "y_true = training_data[label_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32133333333333336"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calc_error(y_pred, y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.16390424 0.18829982 0.44933078 0.24758221 0.14671815 0.34943182]\n",
      " [0.79742173 0.00365631 0.00956023 0.00386847 0.0019305  0.00284091]\n",
      " [0.01104972 0.76416819 0.03441683 0.01547389 0.01351351 0.01988636]\n",
      " [0.         0.00182815 0.43403442 0.00580271 0.00579151 0.00852273]\n",
      " [0.01473297 0.01462523 0.01529637 0.71760155 0.00772201 0.00852273]\n",
      " [0.0092081  0.02010969 0.03441683 0.00773694 0.78957529 0.10511364]\n",
      " [0.00368324 0.00731261 0.02294455 0.00193424 0.03474903 0.50568182]]\n"
     ]
    }
   ],
   "source": [
    "C = confusion_matrix(y_pred, list(y_true))\n",
    "print(C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 1 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-77a8b8a3925e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mC\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 1 with size 6"
     ]
    }
   ],
   "source": [
    "for i in range(1,7): \n",
    "    C[i,i] = 0\n",
    "print(C)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
