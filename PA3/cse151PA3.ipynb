{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PA 3: Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random as rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load in Datasets\n",
    "training_data = pd.read_csv('./data/pa3train.txt', sep=\" \", header=None)\n",
    "label_index = training_data.shape[1] - 1\n",
    "training_data_q1 = training_data.loc[training_data[label_index] <= 2]\n",
    "\n",
    "# Map all 2 and 1 values to 1, -1\n",
    "for i, row in training_data_q1.iterrows(): \n",
    "    if row[label_index] == 1: \n",
    "        row[label_index] = -1\n",
    "    else: \n",
    "        row[label_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_error(Y_pred, Y_label): \n",
    "    # Calculate Error Rate for Predicted Labels\n",
    "    error = [0 for x,y in zip(Y_pred,Y_label) if x != y]\n",
    "    error_rate = len(error)/len(Y_pred)\n",
    "    return error_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class perceptron:\n",
    "    \n",
    "    # Constructor\n",
    "    def __init__(self):\n",
    "        self.w = np.empty((0,0))\n",
    "        self.w_list = []\n",
    "        self.c_list = []\n",
    "    \n",
    "    # Make Predictions\n",
    "    def predict(self, test_data, model):\n",
    "        predictions = []\n",
    "        # Error Checking\n",
    "        if model not in ['normal','voted','averaged']:\n",
    "            print(\"ERROR\")\n",
    "            return []\n",
    "        for i,row in test_data.iterrows():\n",
    "            sample_point = row[:test_data.shape[1]-1]\n",
    "            if model == 'normal': \n",
    "                prediction = np.sign(np.dot(self.w, sample_point))\n",
    "                if prediction == 0: \n",
    "                    prediction = rand.choice([1,-1])\n",
    "                predictions.append(prediction)\n",
    "            elif model == 'voted': \n",
    "                prediction = np.sign(sum(c*(np.sign(np.dot(w,sample_point))) for c, w in zip(self.c_list, self.w_list)))\n",
    "                if prediction == 0: \n",
    "                    prediction = rand.choice([1,-1])\n",
    "                predictions.append(prediction)\n",
    "            elif model == 'averaged': \n",
    "                prediction = np.sign(np.dot((sum(c*w for c, w in zip(self.c_list, self.w_list))),sample_point))\n",
    "                if prediction == 0: \n",
    "                    prediction = rand.choice([1,-1])\n",
    "                predictions.append(prediction)\n",
    "        return predictions\n",
    "    \n",
    "    # Train Classifier\n",
    "    def train(self, data, num_passes): \n",
    "        w_list = []\n",
    "        c_list = []\n",
    "        c = 0\n",
    "        w = np.zeros((data.shape[1]-1,))\n",
    "        for p in range(num_passes): \n",
    "            for i, row in data.iterrows(): \n",
    "                X = row[:data.shape[1]-1]\n",
    "                Y = row[data.shape[1]-1]\n",
    "                if Y*(np.dot(w,np.transpose(X))) <= 0: \n",
    "                    # Adjust decision boundary, otherwise keep it \n",
    "                    w = np.add(w,Y*X)\n",
    "                    w_list.append(w)\n",
    "                    c_list.append(c)\n",
    "                    c = 1\n",
    "                else: \n",
    "                    c += 1\n",
    "        c_list.append(c)\n",
    "        self.w_list = w_list\n",
    "        self.w = w_list[-1]  \n",
    "        self.c_list = c_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "clf1 = perceptron()\n",
    "clf1.train(training_data_q1, 2)\n",
    "y_pred_normal = clf1.predict(training_data_q1, 'normal')\n",
    "y_pred_avg = clf1.predict(training_data_q1, 'averaged')\n",
    "y_pred_voted = clf1.predict(training_data_q1, 'voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR RATES FOR TWO PASSES\n",
      "Error Rate Normal :  0.03944954128440367\n",
      "Error Rate Voted:  0.03761467889908257\n",
      "Error Rate Average:  0.05412844036697248\n"
     ]
    }
   ],
   "source": [
    "y_true = training_data_q1[label_index]\n",
    "print(\"ERROR RATES FOR TWO PASSES\")\n",
    "print(\"Error Rate Normal : \",calc_error(y_pred_normal, y_true))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted, y_true))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf2 = perceptron()\n",
    "clf2.train(training_data_q1, 3)\n",
    "y_pred_normal = clf2.predict(training_data_q1, 'normal')\n",
    "y_pred_avg = clf2.predict(training_data_q1, 'averaged')\n",
    "y_pred_voted = clf2.predict(training_data_q1, 'voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR RATES FOR THREE PASSES\n",
      "Error Rate Normal:  0.02110091743119266\n",
      "Error Rate Voted:  0.031192660550458717\n",
      "Error Rate Average:  0.03669724770642202\n"
     ]
    }
   ],
   "source": [
    "y_true = training_data_q1[label_index]\n",
    "print(\"ERROR RATES FOR THREE PASSES\")\n",
    "print(\"Error Rate Normal: \",calc_error(y_pred_normal, y_true))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted, y_true))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg, y_true))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top (Strong Positive Predictors): \n",
      " 816    393\n",
      "817    469\n",
      "818     78\n",
      "dtype: int64\n",
      "Bottom (Strong Negative Predictors): \n",
      " 0    438\n",
      "1    466\n",
      "2    203\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "w_avg = (sum(c*w for c, w in zip(clf2.c_list, clf2.w_list)))\n",
    "sorted_w = np.argsort(w_avg)\n",
    "print(\"Top (Strong Positive Predictors): \\n\", sorted_w[-3:])\n",
    "print(\"Bottom (Strong Negative Predictors): \\n\", sorted_w[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = perceptron()\n",
    "clf.train(training_data_q1, 4)\n",
    "y_pred_normal = clf.predict(training_data_q1, 'normal')\n",
    "y_pred_avg = clf.predict(training_data_q1, 'averaged')\n",
    "y_pred_voted = clf.predict(training_data_q1, 'voted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR RATES FOR FOUR PASSES\n",
      "Error Rate Normal:  0.01926605504587156\n",
      "Error Rate Voted:  0.022018348623853212\n",
      "Error Rate Average:  0.031192660550458717\n"
     ]
    }
   ],
   "source": [
    "y_true = training_data_q1[label_index]\n",
    "print(\"ERROR RATES FOR FOUR PASSES\")\n",
    "print(\"Error Rate Normal: \",calc_error(y_pred_normal, y_true))\n",
    "print(\"Error Rate Voted: \",calc_error(y_pred_voted, y_true))\n",
    "print(\"Error Rate Average: \",calc_error(y_pred_avg, y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
